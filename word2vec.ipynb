{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import MeCab\n",
    "import mojimoji\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_words(text,stop_word_pass='./Japanese.txt'):\n",
    "#     stopword listを作る\n",
    "    stopword_list=[]\n",
    "#     stop_word_passをreadlineで全て読みstopword_listに格納\n",
    "    with open(stop_word_pass,'r') as f:\n",
    "        stopword_list = f.readlines()\n",
    "#     内包表記でstripにて空白を削除してstopword_listに戻す\n",
    "    stopword_list = [x.strip()for x in stopword_list if x.strip()]\n",
    "    \n",
    "#     形態素解析を始める\n",
    "# MeCab.Tagger()で形態素解析をするための辞書を指定\n",
    "    m = MeCab.Tagger('-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd')\n",
    "#     m.(tagger)parseで、指定した辞書を使用して、文字列やテキストファイルを形態素解析する\n",
    "    m.parse('')\n",
    "#     text=normalize_text(text)\n",
    "    text=mojimoji.zen_to_han(text, kana=False)\n",
    "    m_text = m.parse(text)\n",
    "    basic_words = []\n",
    "#     mecabの出力結果を単語ごとにリスト化\n",
    "    m_text=m_text.split('\\n')\n",
    "    for row in m_text:\n",
    "#     tab区切りで形態素、その品詞などの内容と分かれているので単語部のみ取得\n",
    "        word =row.split(\"\\t\")[0]\n",
    "#         最終行はEOS\n",
    "        if word ==\"EOS\":\n",
    "            break\n",
    "        else:\n",
    "            pos=row.split('\\t')[1]\n",
    "            slice_ =pos.split(',')\n",
    "#             品詞を取得する\n",
    "            parts=slice_[0]\n",
    "            if parts == '記号':\n",
    "                continue\n",
    "                \n",
    "#             活用語の場合は活用指定のない原型を取得する\n",
    "            elif slice_[0] in ('形容詞','動詞') and slice_[-3] not in stopword_list:\n",
    "                basic_words.append(slice_[-3])\n",
    "            \n",
    "#             活用しない語についてはそのままの語を取得する\n",
    "            elif slice_[0] == '名詞' and word not in stopword_list:\n",
    "                basic_words.append(word)\n",
    "    basic_words = ' '.join(basic_words)\n",
    "    return basic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.read_csv('./data/kokkai.csv', header=0)\n",
    "corpus = [x.split(' ') for x in \\\n",
    "          df_input['text'].map(text_to_words).values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['議長',\n",
       " '山東昭子',\n",
       " '君',\n",
       " '国会',\n",
       " '本日',\n",
       " '召集',\n",
       " 'する',\n",
       " 'れる',\n",
       " '会議',\n",
       " '開く',\n",
       " '日程',\n",
       " '議席',\n",
       " '指定',\n",
       " '議長',\n",
       " '本',\n",
       " '院',\n",
       " '規則',\n",
       " '十四条',\n",
       " '規定',\n",
       " '皆さん',\n",
       " '議席',\n",
       " '議席',\n",
       " '指定',\n",
       " 'いたす']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル作成\n",
    "モデル作成を行い、このモデルを保管する。 　\n",
    "- size→次元数。\n",
    "- min_count→何回以上出現した単語を対象にするか。\n",
    "- window→前後どこまでを対象とするか。\n",
    "- sg→1はski-gramそれ以外はCBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(corpus, size=200, min_count=3, window=5, sg=1)\n",
    "model.save('./data/kokkai_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベクトルの出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['コロナ'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76913106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-2b0c09197093>:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  print(model.similarity('コロナ', 'アフター'))\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('コロナ', 'アフター'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ウイズコロナ', 0.9622336626052856),\n",
       " ('新たな日常', 0.8852411508560181),\n",
       " ('日本社会', 0.8806211352348328),\n",
       " ('傷む', 0.8753450512886047),\n",
       " ('経済政策', 0.8734216690063477),\n",
       " ('模索', 0.858197033405304),\n",
       " ('共存', 0.8571842908859253),\n",
       " ('局面', 0.8555092811584473),\n",
       " ('持ち直し', 0.8549432158470154),\n",
       " ('ピンチ', 0.8532471060752869)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar(positive=['アフター'], topn=10) #top10\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
